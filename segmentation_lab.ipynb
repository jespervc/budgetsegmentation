{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "segmentation_lab.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6J27CzM_Y0F"
      },
      "source": [
        "# Image Segmentation Laboratory\n",
        "\n",
        "1695 2D multi-channel brain MR images were collected from the Brain Tumor Segmentation ([BraTS](http://braintumorsegmentation.org/)) Challenge 2020 training set.\n",
        "Each available file here contains 4 MR images (T1, T2, T1ce, and FLAIR), and its true segmentation.\n",
        "\n",
        "The aim of this assignment is to develop a [U-Net](https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/) convolutional neural network able to segment and highlight the brain tumour from such images.\n",
        "\n",
        "Let's start making sure that our script can see the graphics card that will be used. The graphics cards will perform all the time consuming convolutions in every training iteration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DeCiRGEE_Y0G",
        "outputId": "b50dacdb-aca4-4606-a42f-2a882e81e20a"
      },
      "source": [
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import tensorflow as tf\n",
        "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "\n",
        "########## YOUR CODE ############\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#%cd /content/drive/My\\ Drive/Colab\\ Notebooks\n",
        "#%%writefile utils.py\n",
        "!wget -O seg_lab.zip https://tiny.cc/vxt7gz\n",
        "!unzip seg_lab.zip;\n",
        "!conda env create -f env.ymlNN\n",
        "##########    END    ############"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "--2021-07-19 11:38:26--  https://tiny.cc/vxt7gz\n",
            "Resolving tiny.cc (tiny.cc)... 157.245.113.153\n",
            "Connecting to tiny.cc (tiny.cc)|157.245.113.153|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://liuonline-my.sharepoint.com/:u:/g/personal/davab27_liu_se/ESibIRhFBoZFt07YBsvwt9QBlDQfyFC_py9Q8SpF3hRQmA?download=1 [following]\n",
            "--2021-07-19 11:38:26--  https://liuonline-my.sharepoint.com/:u:/g/personal/davab27_liu_se/ESibIRhFBoZFt07YBsvwt9QBlDQfyFC_py9Q8SpF3hRQmA?download=1\n",
            "Resolving liuonline-my.sharepoint.com (liuonline-my.sharepoint.com)... 13.107.136.9, 13.107.138.9\n",
            "Connecting to liuonline-my.sharepoint.com (liuonline-my.sharepoint.com)|13.107.136.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /personal/davab27_liu_se/Documents/seg_lab.zip?originalPath=aHR0cHM6Ly9saXVvbmxpbmUtbXkuc2hhcmVwb2ludC5jb20vOnU6L2cvcGVyc29uYWwvZGF2YWIyN19saXVfc2UvRVNpYklSaEZCb1pGdDA3WUJzdnd0OVFCbERRZnlGQ19weTlROFNwRjNoUlFtQT9ydGltZT05dG5YdWFsSzJVZw [following]\n",
            "--2021-07-19 11:38:27--  https://liuonline-my.sharepoint.com/personal/davab27_liu_se/Documents/seg_lab.zip?originalPath=aHR0cHM6Ly9saXVvbmxpbmUtbXkuc2hhcmVwb2ludC5jb20vOnU6L2cvcGVyc29uYWwvZGF2YWIyN19saXVfc2UvRVNpYklSaEZCb1pGdDA3WUJzdnd0OVFCbERRZnlGQ19weTlROFNwRjNoUlFtQT9ydGltZT05dG5YdWFsSzJVZw\n",
            "Reusing existing connection to liuonline-my.sharepoint.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27292866 (26M) [application/x-zip-compressed]\n",
            "Saving to: ‘seg_lab.zip’\n",
            "\n",
            "seg_lab.zip         100%[===================>]  26.03M  11.4MB/s    in 2.3s    \n",
            "\n",
            "2021-07-19 11:38:30 (11.4 MB/s) - ‘seg_lab.zip’ saved [27292866/27292866]\n",
            "\n",
            "Archive:  seg_lab.zip\n",
            "replace Data/Masks/mask9.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "/bin/bash: conda: command not found\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfDf7E12_Y0H"
      },
      "source": [
        "## Load data\n",
        "Open the file `utils.py` and get familiar with its functions.\n",
        "\n",
        "The images are saved as `.npy` files in the folder `Data2D_BRATS_multiseg`. \n",
        "Each file contains 5 channels:\n",
        "- channels 0 to 3 contain T1, T2, T1ce, and FLAIR modalities;\n",
        "- channel 4 contains the true segmentation.\n",
        "\n",
        "Use the functions `load_img()` and `visualize()` in `utils.py` to load and visualize one image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFhtnvKC_Y0I",
        "outputId": "a4b0176b-6443-4e7e-e848-fbfbc377a240"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks/Data/Images\n",
        "from utils import * \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "\n",
        "Nclasses = 2\n",
        "list_images = sorted(glob.glob('./Data_BraTS/*.npy'))\n",
        "Nim = len(list_images)\n",
        "\n",
        "print('The image dataset contatins: {} images.'.format(Nim))\n",
        "########## YOUR CODE ############\n",
        "#x = load_img('./Data2D_BRATS_multiseg/',2)\n",
        "#x = load_img(list_images[1],Nclasses)\n",
        "\n",
        "\n",
        "##########    END    ############"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Data/Images\n",
            "The image dataset contatins: 0 images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FdXGhDb_Y0I"
      },
      "source": [
        "##  Split data into training, validation and testing\n",
        "\n",
        "We will use the `DataGenerator` class to feed the images to the model during training. This class streams the data directly from the `.npy` files, so we do not need to load the images explicitly. Therefore, we will work with the file list instead, stored in `list_images`.\n",
        "\n",
        "Split `list_images` into training (`list_train`), validation (`list_valid`) and testing (`list_test`) indexes sets such as they are approximately the 70%, 20%, and 10% of the whole list, using [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
        "\n",
        "Use them to create 3 `DataGenerators`, one for each different set, with the default options."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn59tsyh_Y0J"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "########## YOUR CODE ############\n",
        "# split the data\n",
        "\n",
        "# create the three generators\n",
        "\n",
        "##########    END    ############\n",
        "\n",
        "Ntrain = len(list_train)\n",
        "Nvalid = len(list_valid)\n",
        "Ntest = len(list_test)\n",
        "\n",
        "print('The training, validation, and testing set have {} ({:.2f}%), {} ({:.2f}%) and {} ({:.2f}%) images respectively.'\n",
        "      .format(Ntrain, 100*Ntrain/Nim, Nvalid, 100*Nvalid/Nim, Ntest, 100*Ntest/Nim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8BViPqI_Y0J"
      },
      "source": [
        "### Data visualization\n",
        "Let's use now the `DataGenerator` class to load and visualize a batch of data.\n",
        "\n",
        "- `len(DataGenerator)` returns the lenghts of the generator, that is the total number of batches;\n",
        "- `DataGenerator[i]` returns the i-th batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Daq4dfe4_Y0K"
      },
      "source": [
        "# choose a random batch index\n",
        "idx = np.random.randint(len(train_gen))\n",
        "\n",
        "# load the idx-th batch\n",
        "Xbatch, ybatch = train_gen[idx]\n",
        "print('An image batch has shape: {}'.format(Xbatch.shape))\n",
        "print('A target batch has shape: {}'.format(ybatch.shape))\n",
        "\n",
        "# visualize few data samples \n",
        "for i in range(4):\n",
        "    visualize(Xbatch[i], ybatch[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq-x5xsz_Y0K"
      },
      "source": [
        "## Balance the classes\n",
        "\n",
        "Extract and print here below the class weights that belong to the training set using the function:\n",
        "\n",
        "`class_weights = n_samples / (n_classes * np.bincount(y))`.\n",
        "\n",
        "Pay attention that the `np.bincount(y))` function wants its input as a flattened array and its elements as integers. In order to speed up the calculation, you can calculate the paramenters just on the first 10, 20 or 30 batches.\n",
        "\n",
        "In the end, store the weights in `class_weights` and convert them into `float32` dtype to be used properly in the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4-FY5w2_Y0L"
      },
      "source": [
        "Y = []\n",
        "n = 0\n",
        "\n",
        "for _, ybatch in train_gen:\n",
        "\n",
        "    # concatenate in Y the n-th flattened target batch\n",
        "    Y = np.concatenate((Y, np.argmax(ybatch, axis=-1).flatten()))\n",
        "    \n",
        "    # interrupt the cycle\n",
        "    n += 1\n",
        "    if n == 30:\n",
        "        break\n",
        "\n",
        "Y = Y.astype('int64')\n",
        "\n",
        "########## YOUR CODE ############\n",
        "# calcuate the class weights\n",
        "\n",
        "##########    END    ############\n",
        "\n",
        "print('The class weights for the four different classes are respectively:\\n{}'.format(class_weights))\n",
        "print(class_weights.dtype)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrREX1KY_Y0L"
      },
      "source": [
        "<b>Question:</b> What can you say about the class weights just calculated? Are they as you expected? \n",
        "\n",
        "<b>Answer:</b>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MZ7RqCM_Y0M"
      },
      "source": [
        "## Pre-processing\n",
        "Perform image normalization only on the brain area (the background must stay with 0 value), tips at [link](http://cs231n.github.io/neural-networks-2/)). \n",
        "\n",
        "Complete the function `norm_brain` in `utils.py`. Then, create new `DataGenerator` objects, setting `norm=True`. Set also `batch_size=1` for the test `DataGenerator` only.\n",
        "\n",
        "Afterwards, plot a few normalized images to see the difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfkGezLp_Y0N"
      },
      "source": [
        "########## YOUR CODE ############\n",
        "# create new DataGenerators\n",
        "\n",
        "# visualize a few data samples\n",
        "\n",
        "##########    END    ############"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB8LIJZ0_Y0N"
      },
      "source": [
        "## U-Net\n",
        "Finish this code to create the U-Net architecture.\n",
        "\n",
        "Relevant [Keras](https://www.tensorflow.org/api_docs/python/tf/keras) layers are:\n",
        "\n",
        "`Conv2D`, performs 2D convolutions with a number of filters with a certain size. Use `he_normal` weights initialization, and `same` padding;\n",
        "\n",
        "`BatchNormalization`, normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1;\n",
        "\n",
        "`Activation`, i.e. `relu` activation function sets to 0 all the negatives values;\n",
        "\n",
        "`MaxPool2D`, saves the max for a given pool size, results in down sampling;\n",
        "\n",
        "`Conv2DTranspose`, increases the resolution by a certain factor, through interpolation;\n",
        "\n",
        "`Dropout`, randomly sets input units to 0 with a frequency of `rate` at each step during training time, which helps prevent overfitting.\n",
        "\n",
        "`concatenate`, concatenate a list of inputs.\n",
        "\n",
        "Moreover, the useful list methods for this lab are:\n",
        "\n",
        "`.append()`, adds an element to the end of a list;\n",
        "\n",
        "`.pop()`, gets an element from the end of a list.\n",
        "\n",
        "![Unet architecture](https://github.com/jespervc/budgetsegmentation/blob/main/Unet_architecture.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baKTBZiS_Y0O"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, Activation, BatchNormalization, MaxPool2D, Conv2DTranspose, Dropout, concatenate\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "class UNet(object):\n",
        "    \n",
        "    def __init__(self, img_size, Nclasses, class_weights, model_name='myWeights.h5', Nfilter_start=64, depth=3):\n",
        "        self.img_size = img_size\n",
        "        self.Nclasses = Nclasses\n",
        "        self.class_weights = class_weights\n",
        "        self.model_name = model_name\n",
        "        self.Nfilter_start = Nfilter_start\n",
        "        self.depth = depth\n",
        "\n",
        "        inputs = Input(img_size)\n",
        "    \n",
        "        def dice(y_true, y_pred, w=self.class_weights):\n",
        "            y_true = tf.convert_to_tensor(y_true, 'float32')\n",
        "            y_pred = tf.convert_to_tensor(y_pred, 'float32')\n",
        "\n",
        "            num = 2 * tf.reduce_sum(tf.reduce_sum(y_true*y_pred, axis=[0,1,2])*w)\n",
        "            den = tf.reduce_sum(tf.reduce_sum(y_true+y_pred, axis=[0,1,2])*w) + 1e-5\n",
        "\n",
        "            return num/den\n",
        "\n",
        "        def diceLoss(y_true, y_pred):\n",
        "            return 1-dice(y_true, y_pred)\n",
        "        \n",
        "        \n",
        "        ########## YOUR CODE ############\n",
        "        \n",
        "        # This is a help function that performs 2 convolutions, each followed by batch normalization\n",
        "        # and ReLu activations, Nf is the number of filters, filter size (3 x 3)\n",
        "        def convs(layer, Nf):\n",
        "            \n",
        "            return x\n",
        "            \n",
        "        # This is a help function that defines what happens in each layer of the encoder (downstream),\n",
        "        # which calls \"convs\" and then Maxpooling (2 x 2). Save each layer for later concatenation in the upstream.\n",
        "        def encoder_step(layer, Nf):\n",
        "            \n",
        "            return y, x\n",
        "            \n",
        "        # This is a help function that defines what happens in each layer of the decoder (upstream),\n",
        "        # which contains upsampling (2 x 2), 2D convolution (2 x 2), batch normalization, concatenation with \n",
        "        # corresponding layer (y) from encoder, and lastly \"convs\"\n",
        "        def decoder_step(layer, layer_to_concatenate, Nf):\n",
        "            \n",
        "            return x\n",
        "        \n",
        "        layers_to_concatenate = []\n",
        "        x = inputs\n",
        "        \n",
        "        # Make encoder with 'self.depth' layers, \n",
        "        # note that the number of filters in each layer will double compared to the previous \"step\" in the encoder\n",
        "        for d in range(self.depth-1):\n",
        "            \n",
        "            \n",
        "            \n",
        "        # Make bridge, that connects encoder and decoder using \"convs\" between them. \n",
        "        # Use Dropout before and after the bridge, for regularization. Use drop probability of 0.2.\n",
        "        \n",
        "        \n",
        "        \n",
        "        # Make decoder with 'self.depth' layers, \n",
        "        # note that the number of filters in each layer will be halved compared to the previous \"step\" in the decoder\n",
        "        for d in range(self.depth-2, -1, -1):\n",
        "            \n",
        "            \n",
        "        ##########    END    ############\n",
        "        \n",
        "        # Make classification (segmentation) of each pixel, using convolution with 1 x 1 filter\n",
        "        final = Conv2D(filters=self.Nclasses, kernel_size=(1,1), activation = 'softmax')(x)\n",
        "        \n",
        "        # Create model\n",
        "        self.model = Model(inputs=inputs, outputs=final)\n",
        "        self.model.compile(loss=diceLoss, optimizer=Adam(lr=1e-4), metrics=['accuracy',dice]) \n",
        "        \n",
        "    def train(self, train_gen, valid_gen, nEpochs):\n",
        "        print('Training process:')       \n",
        "        callbacks = [ModelCheckpoint(self.weights_name, save_best_only=True, save_weights_only=True),\n",
        "                     EarlyStopping(patience=10)]\n",
        "        \n",
        "        history = self.model.fit(train_gen, validation_data=valid_gen, epochs=nEpochs, callbacks=callbacks)\n",
        "\n",
        "        return history    \n",
        "    \n",
        "    def evaluate(self, test_gen):\n",
        "        print('Evaluation process:')\n",
        "        loss, acc, dice = self.model.evaluate(test_gen)\n",
        "        print('Accuracy: {:.4f}'.format(acc*100))\n",
        "        print('Dice: {:.4f}'.format(dice*100))\n",
        "        return acc, dice\n",
        "    \n",
        "    def predict(self, X):\n",
        "        y_pred = self.model.predict(X)\n",
        "        return y_pred\n",
        "    \n",
        "    def calculate_metrics(self, y_true_flat, y_pred_flat):\n",
        "        ########## YOUR CODE ############\n",
        "        # be sure that the inputs are binary.\n",
        "        # calculate the confusion matrix using tf.math.confusion_matrix()\n",
        "\n",
        "        # calculate the accuracy and Dice using. \n",
        "        # Set to np.nan the value for Dice when the confusion matrix has only true negatives.\n",
        "    \n",
        "        return 0, 0\n",
        "        \n",
        "        ##########    END    ############\n",
        "    \n",
        "    def get_metrics(self, generator):\n",
        "        ''' This function calculates the metrics accuracy and Dice for each image contained in the input generator.\n",
        "        '''\n",
        "        Nim = len(generator)*generator.batch_size\n",
        "        ACC = np.empty((Nim, self.Nclasses))\n",
        "        DICE = np.empty((Nim, self.Nclasses))\n",
        "        n = 0\n",
        "        for i in range(len(generator)):\n",
        "            X_batch, y_batch = generator[i]\n",
        "            y_pred = self.model.predict(X_batch)\n",
        "            y_pred = to_categorical(tf.argmax(y_pred, axis=-1), self.Nclasses)\n",
        "\n",
        "            for c in range(Nclasses):\n",
        "                y_true_flat = tf.reshape(y_batch[0,:,:,c], (160*192,))\n",
        "                y_pred_flat = tf.reshape(y_pred[0,:,:,c], (160*192,))              \n",
        "\n",
        "                acc, dice = self.calculate_metrics(y_true_flat, y_pred_flat)\n",
        "                ACC[n,c] = acc\n",
        "                DICE[n,c] = dice\n",
        "\n",
        "            n+=1\n",
        "\n",
        "        return ACC, DICE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pja1syMu_Y0R"
      },
      "source": [
        "### Visualize the network\n",
        "Initialize your U-Net `net` and print out the model using `net.model.summary()`.\n",
        "If it prints out error you may have implemented the architecture incorrectly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "XV2m8_4G_Y0T"
      },
      "source": [
        "img_size = (160, 192, 4)\n",
        "net = UNet(img_size, Nclasses, class_weights, weights_name='myWeights_multiseg.h5')\n",
        "net.model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jcDdxI9_Y0U"
      },
      "source": [
        "If you could print the summary, then you can also visualize the network"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "OZsvWuWI_Y0U"
      },
      "source": [
        "tf.keras.utils.plot_model(net.model, to_file='model.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZYEf4QC_Y0U"
      },
      "source": [
        "### Train the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBy73ycA_Y0U"
      },
      "source": [
        "Use the method `train()`of the `UNet` class to train your network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "L3XU6G7Z_Y0V"
      },
      "source": [
        "results = net.train(train_gen, valid_gen, nEpochs=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqcyUfTS_Y0V"
      },
      "source": [
        "Now use the function `plot_trends()`in `utils.py` to visualize the accuracy, dice, and dice loss trends dunring the training epochs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ySALQ0k_Y0V"
      },
      "source": [
        "plot_trends(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uzk-_ck_Y0V"
      },
      "source": [
        "### Evaluate the network\n",
        "Use the method `evaluate()` of the `UNet` class to calculate the mean performance of the network on the test set.\n",
        "\n",
        "Compleate the method `calculate_metrics()` used in `get_metrics()` in the `UNet` class to calcualte the accuracy and Dice score per each class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "erJnxxYI_Y0W"
      },
      "source": [
        "acc, dice = net.evaluate(test_gen)\n",
        "ACC, DICE = net.get_metrics(test_gen)\n",
        "     \n",
        "ACC_avg = np.nanmean(ACC, axis=0)\n",
        "DICE_avg = np.nanmean(DICE, axis=0)\n",
        "ACC_std = np.nanstd(ACC, axis=0)\n",
        "DICE_std = np.nanstd(DICE, axis=0)\n",
        "\n",
        "print('\\nThe network detects:')\n",
        "for c in range(Nclasses):\n",
        "    print('- class {} with accuracy: {:.4f} +/- {:.4f}, and dice score: {:.4f} +/- {:.4f}'\n",
        "          .format(c, ACC_avg[c], ACC_std[c], DICE_avg[c], DICE_std[c]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1GnrCyg_Y0W"
      },
      "source": [
        "**Question:** Do you see any differences? Can you explain why?\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nJYXb8F_Y0W"
      },
      "source": [
        "### Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUQkXv8z_Y0W"
      },
      "source": [
        "idx = np.random.randint(len(test_gen))\n",
        "X_batch, y_batch = test_gen[idx]\n",
        "y_pred = net.predict(X_batch)\n",
        "y_pred = to_categorical(np.argmax(y_pred, axis=-1), Nclasses)\n",
        "\n",
        "print('TRUE')\n",
        "visualize(X_batch[0], y_batch[0])\n",
        "print('PREDICTION')\n",
        "visualize(X_batch[0], y_pred[0])\n",
        "\n",
        "print('This segmentation prediction has:')\n",
        "for c in range(Nclasses):\n",
        "    y_true_flat = tf.reshape(y_batch[0,:,:,c], (160*192,))\n",
        "    y_pred_flat = tf.reshape(y_pred[0,:,:,c], (160*192,))\n",
        "    \n",
        "    acc, dice = net.calculate_metrics(y_true_flat, y_pred_flat)\n",
        "    print('- class {} with accuracy: {:.4f}, and dice score: {:.4f}'\n",
        "          .format(c, acc, dice))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jjFL-6p_Y0X"
      },
      "source": [
        "## Image Augmentation\n",
        "\n",
        "Before start using the `DataGenerator` for the augmentation, let's get familiar with the techniques we are going to use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74PZHAcA_Y0X"
      },
      "source": [
        "### Flipping\n",
        "A very easy and used augmentation technique is flipping. \n",
        "The output image is simply the reflection of the original one along one of the two main axis, so the image can be flipped from left to right or up and down. Luckily for you, [`tf.image`](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/image) provides this function. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baCHO-xR_Y0X"
      },
      "source": [
        "from tensorflow.image import flip_left_right, flip_up_down\n",
        "\n",
        "idx = np.random.randint(len(train_gen))\n",
        "Xbatch, ybatch = train_gen[idx]\n",
        "im, gt = Xbatch[0], ybatch[0]\n",
        "\n",
        "im_lr, gt_lr = flip_left_right(im), flip_left_right(gt)\n",
        "im_ud, gt_ud = flip_up_down(im), flip_up_down(gt)\n",
        "\n",
        "print('TRUE')\n",
        "visualize(im, gt)\n",
        "print('LEFT-RIGHT')\n",
        "visualize(im_lr, gt_lr)\n",
        "print('UP-DOWN')\n",
        "visualize(im_ud, gt_ud)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5U3rXik_Y0Y"
      },
      "source": [
        "### Affine Transormation\n",
        "Affine transformation is that trasformation that takes into account translation, rotation, shear and zoom spatial transformation at the same time. \n",
        "\n",
        "Luckily for us, Keras provides such function: [`tf.keras.preprocessing.image`](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/preprocessing/image)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Drlls9Wp_Y0Y"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import apply_affine_transform\n",
        "\n",
        "rows, columns, channels = im.shape\n",
        "\n",
        "########## YOUR CODE ############\n",
        "# try some parameters:\n",
        "# rotation angle \n",
        "alpha = 0\n",
        "\n",
        "# number of pixels to shift the image along the 2 axis\n",
        "Tx, Ty = 0, 0\n",
        "\n",
        "# shear angle in degrees\n",
        "beta = 0\n",
        "\n",
        "# zoom percentage to impose alog the 2 axis\n",
        "Zx, Zy = 1, 1\n",
        "##########    END    ############\n",
        "\n",
        "print('The augmentation is goint to use:')\n",
        "print('  - Rotation of {:.2f} degrees;'.format(alpha))\n",
        "print('  - Translation of {} and {} along the x and y axis respectively;'.format(Tx, Ty))\n",
        "print('  - Shear of {:.2f} degrees;'.format(beta))\n",
        "print('  - Zoom of {:.2f} and {:.2f} along the x and y axis respectively.'.format(Zx, Zy))\n",
        "print(' ')\n",
        "\n",
        "# apply affine tranformation to the image\n",
        "im2 = apply_affine_transform(im, \n",
        "                             theta=alpha,      # rotation\n",
        "                             tx=Tx, ty=Ty,     # translation\n",
        "                             shear=beta,       # shear\n",
        "                             zx=Zx, zy=Zy,     # zoom\n",
        "                             row_axis=0, col_axis=1, channel_axis=2, \n",
        "                             fill_mode='constant', cval=0.0, \n",
        "                             order=1)\n",
        "\n",
        "# apply affine tranformation to the target\n",
        "gt2 = apply_affine_transform(gt, \n",
        "                             theta=alpha,      # rotation\n",
        "                             tx=Tx, ty=Ty,     # translation\n",
        "                             shear=beta,       # shear\n",
        "                             zx=Zx, zy=Zy,     # zoom\n",
        "                             row_axis=0, col_axis=1, channel_axis=2, \n",
        "                             fill_mode='constant', cval=0.0, \n",
        "                             order=0)\n",
        "\n",
        "print('TRUE')\n",
        "visualize(im, gt)\n",
        "print('AUGMENTED')\n",
        "visualize(im2, gt2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW1k083O_Y0Z"
      },
      "source": [
        "Now that you got familiar with such functions, edit the `augmentation()` function in `utils.py`.\n",
        "\n",
        "The function takes as input one multi-channel MRI, its segmentation and an array `do` made of 0s and 1s. It applies the $i$-th augmentation technique if `do[i] == 1`, otherwise it does not.\n",
        "\n",
        "Once you have completed this task, create a training and validation `DataGenerator`s (`train_gen_aug` and `valid_gen_aug`) that apply image normalization and augmentation. So, the generators should have as inputs: `norm=True` and `augmentation=True`.\n",
        "\n",
        "Visualize also a few samples to understand better what is going on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amHZOMk4_Y0a"
      },
      "source": [
        "########## YOUR CODE ############\n",
        "# create new DataGenerators\n",
        "\n",
        "# visualize a few data samples\n",
        "\n",
        "##########    END    ############"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_-SOCzb_Y0a"
      },
      "source": [
        "### Train the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "h56_71Xw_Y0a"
      },
      "source": [
        "net2 = unet(img_size, Nclasses, class_weights, weights_name='myWeightsAug_multiseg.h5')\n",
        "results = net2.train(train_gen_aug, valid_gen_aug, nEpochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki4ELKLl_Y0b"
      },
      "source": [
        "plot_trends(results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SbqwI-9_Y0b"
      },
      "source": [
        "### Evaluate the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxK0q5xE_Y0b"
      },
      "source": [
        "acc, dice = net2.evaluate(test_gen)\n",
        "ACC, DICE = net2.get_metrics(test_gen)\n",
        "\n",
        "ACC_avg = np.nanmean(ACC, axis=0)\n",
        "DICE_avg = np.nanmean(DICE, axis=0)\n",
        "ACC_std = np.nanstd(ACC, axis=0)\n",
        "DICE_std = np.nanstd(DICE, axis=0)\n",
        "\n",
        "print('\\nThe network detects:')\n",
        "for c in range(Nclasses):\n",
        "    print('- class {} with accuracy: {:.4f} +/- {:.4f}, and dice score: {:.4f} +/- {:.4f}'\n",
        "          .format(c, ACC_avg[c], ACC_std[c], DICE_avg[c], DICE_std[c]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qhPkTTH_Y0c"
      },
      "source": [
        "### Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmXtZgDK_Y0c"
      },
      "source": [
        "idx = np.random.randint(len(test_gen))\n",
        "X_batch, y_batch = test_gen[idx]\n",
        "y_pred = net.predict(X_batch)\n",
        "y_pred = to_categorical(np.argmax(y_pred, axis=-1), Nclasses)\n",
        "\n",
        "print('TRUE')\n",
        "visualize(X_batch[0], y_batch[0])\n",
        "print('PREDICTION')\n",
        "visualize(X_batch[0], y_pred[0])\n",
        "\n",
        "print('This segmentation prediction has:')\n",
        "for c in range(Nclasses):\n",
        "    y_true_flat = tf.reshape(y_batch[0,:,:,c], (160*192,))\n",
        "    y_pred_flat = tf.reshape(y_pred[0,:,:,c], (160*192,))\n",
        "    \n",
        "    acc, dice = net.calculate_metrics(y_true_flat, y_pred_flat)\n",
        "    print('- class {} with accuracy: {:.4f}, and dice score: {:.4f}'\n",
        "          .format(c, acc, dice))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK1YoIyK_Y0c"
      },
      "source": [
        "**Question:** Based on your quantitative and qualitative results, do you think it is better training with or without augmentation?\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHrjCp_w_Y0d"
      },
      "source": [
        "## Multi-class segmentation (optional)\n",
        "Implement multi-class segmentaiton for brain tumors. Set `Nclasses=4` at the beginning of the main code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyl2q1Q8_Y0d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}